#!/bin/env python3

import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, urldefrag
import language_tool_python
from rich.console import Console
from rich.progress import Progress, SpinnerColumn, TextColumn, BarColumn, TimeElapsedColumn
from rich.table import Table
from collections import deque
import re
import sys
import os

BASE_URL = "http://localhost/oo2_gui/"
WHITELIST_FILE = ".tests/whitelisted_words"

console = Console()

def is_valid_url(url):
    # Exclude urls containing 'share' or 'usage_stats'
    if 'share' in url or 'usage_stats' in url:
        return False
    # Accept only URLs that start with BASE_URL or relative URLs
    return url.startswith(BASE_URL) or not bool(urlparse(url).netloc)

def extract_links(soup, base_url):
    links = set()
    for a in soup.find_all('a', href=True):
        href = a['href'].strip()
        if not href or href.startswith('#') or href.startswith('mailto:') or href.startswith('javascript:'):
            continue
        full_url = urljoin(base_url, href)
        if is_valid_url(full_url):
            links.add(full_url)
    return links

def remove_code_and_pre_blocks(soup):
    # Entfernt <code> und <pre> Elemente aus dem BeautifulSoup-Objekt
    for tag in soup.find_all(['code', 'pre']):
        tag.decompose()

def remove_latex_code(text):
    # Entfernt Latex-Code zwischen $$ ... $$
    # Multiline möglich, non-greedy
    cleaned_text = re.sub(r'\$\$.*?\$\$', '', text, flags=re.DOTALL)
    return cleaned_text

def extract_main_content_segments(soup):
    main_div = soup.find('div', id='mainContent')
    if not main_div:
        return []
    remove_code_and_pre_blocks(main_div)
    segments = []

    # Sammle Überschriften und Absätze getrennt, jeweils als eigene Segmente
    for tag in main_div.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li']):
        text = tag.get_text(separator=' ', strip=True)
        if text:
            text = remove_latex_code(text)
            segments.append(text)

    return segments

def load_whitelisted_words(filepath):
    if not os.path.isfile(filepath):
        console.print(f"[yellow]Whitelist file not found: {filepath}[/yellow]")
        return set()
    try:
        with open(filepath, "r", encoding="utf-8") as f:
            words = set()
            for line in f:
                clean_line = line.strip()
                if clean_line and not clean_line.startswith('#'):
                    words.add(clean_line)
            return words
    except Exception as e:
        console.print(f"[red]Error reading whitelist file {filepath}: {e}[/red]")
        return set()

def check_text_segments(segments, tool, whitelist):
    all_matches = []
    for segment in segments:
        try:
            matches = tool.check(segment)
            # Filter wie vorher
            filtered_matches = []
            for match in matches:
                is_whitelisted = False
                try:
                    if hasattr(match, 'offset') and hasattr(match, 'errorLength'):
                        error_word = segment[match.offset:match.offset + match.errorLength].strip()
                        if error_word in whitelist:
                            is_whitelisted = True
                    else:
                        context_words = re.findall(r'\w+', match.context.lower())
                        for ww in whitelist:
                            if ww.lower() in context_words:
                                is_whitelisted = True
                                break
                except Exception:
                    pass

                if not is_whitelisted:
                    for replacement in match.replacements:
                        if replacement.lower() in whitelist:
                            is_whitelisted = True
                            break

                if not is_whitelisted:
                    filtered_matches.append(match)

            all_matches.extend(filtered_matches)

        except Exception as e:
            console.print(f"[red]Error during language check: {e}[/red]")

    return all_matches

def check_text(text, tool, whitelist):
    try:
        matches = tool.check(text)
        # Filter out matches which are whitelisted words (in context or replacements)
        filtered_matches = []
        for match in matches:
            is_whitelisted = False
            try:
                if hasattr(match, 'offset') and hasattr(match, 'errorLength'):
                    error_word = text[match.offset:match.offset + match.errorLength].strip()
                    if error_word in whitelist:
                        is_whitelisted = True
                else:
                    context_words = re.findall(r'\w+', match.context.lower())
                    for ww in whitelist:
                        if ww.lower() in context_words:
                            is_whitelisted = True
                            break
            except Exception:
                pass

            if not is_whitelisted:
                for replacement in match.replacements:
                    if replacement.lower() in whitelist:
                        is_whitelisted = True
                        break

            if not is_whitelisted:
                filtered_matches.append(match)

        return filtered_matches

    except Exception as e:
        console.print(f"[red]Error during language check: {e}[/red]")
        return []

def crawl_and_check(base_url):
    visited = set()
    queue = deque([base_url])
    results = {}

    tool = language_tool_python.LanguageTool('en-US')

    # Whitelist laden, aber NICHT dem LanguageTool hinzufügen!
    whitelist = load_whitelisted_words(WHITELIST_FILE)
    # KEIN tool.add_ignore(word) MEHR!

    with Progress(
        SpinnerColumn(),
        TextColumn("[progress.description]{task.description}"),
        BarColumn(),
        TimeElapsedColumn(),
        transient=True,
        console=console
    ) as progress:
        crawl_task = progress.add_task("Crawling and checking pages...", total=0)
        pages_checked = 0

        while queue:
            current_url = queue.popleft()

            # Normalize URL by removing fragment before checking visited
            normalized_url, _ = urldefrag(current_url)

            if normalized_url in visited:
                continue
            visited.add(normalized_url)

            progress.update(crawl_task, description=f"[cyan]Fetching[/cyan] {current_url}")
            try:
                response = requests.get(current_url, timeout=15)
                if response.status_code != 200:
                    console.print(f"[yellow]Warning: Got status {response.status_code} for URL {current_url}[/yellow]")
                    continue
                soup = BeautifulSoup(response.text, 'html.parser')

                new_links = extract_links(soup, current_url)
                for link in new_links:
                    norm_link, _ = urldefrag(link)
                    if norm_link not in visited and link not in queue:
                        queue.append(link)

                    segments = extract_main_content_segments(soup)
                    if not segments:
                        console.print(f"[yellow]No mainContent segments found at {current_url}[/yellow]")
                        results[current_url] = []
                    else:
                        matches = check_text_segments(segments, tool, whitelist)
                        results[current_url] = matches

                pages_checked += 1
                progress.update(crawl_task, total=pages_checked + len(queue), advance=1)

            except requests.RequestException as e:
                console.print(f"[red]Failed to fetch {current_url}: {e}[/red]")
            except Exception as e:
                console.print(f"[red]Unexpected error on {current_url}: {e}[/red]")

    return results

def print_results(results):
    table = Table(title="LanguageTool Issues Found", show_lines=True)
    table.add_column("URL", style="cyan", no_wrap=True)
    table.add_column("Issue", style="red")
    table.add_column("Message")
    table.add_column("Incorrect Text", style="magenta")
    table.add_column("Suggestions", style="green")

    total_issues = 0
    for url, matches in results.items():
        if not matches:
            continue
        for match in matches:
            issue = match.ruleId or "N/A"
            message = match.message or ""
            incorrect_text = match.context if hasattr(match, 'context') else ""
            replacement_suggestions = ", ".join(match.replacements) if match.replacements else "None"
            table.add_row(url, issue, message, incorrect_text, replacement_suggestions)
            total_issues += 1

    if total_issues == 0:
        console.print("[green]No language issues found on any crawled page.[/green]")
    else:
        console.print(table)

def main():
    console.print(f"[bold]Starting crawl on:[/bold] {BASE_URL}\n")
    results = crawl_and_check(BASE_URL)
    print_results(results)
    console.print(f"\n[bold]Crawled {len(results)} pages.[/bold]")

if __name__ == "__main__":
    try:
        main()
    except KeyboardInterrupt:
        console.print("\n[red]Script interrupted by user (CTRL+C).[/red]")
        sys.exit(1)
