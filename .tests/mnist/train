#!/usr/bin/env python3

import sys
import os
import platform
import shutil
import subprocess
import signal
import traceback
from pathlib import Path
import hashlib
import time

start_time = time.time()

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

SCRIPT_DIR = Path(__file__).resolve().parent

try:
    import venv
except ModuleNotFoundError:
    print("venv not found. Is python3-venv installed?")
    sys.exit(1)

clusterhost = os.environ.get("CLUSTERHOST", "").encode()
hash_suffix = hashlib.sha256(clusterhost).hexdigest()[:10]
VENV_PATH = SCRIPT_DIR / f".torch_venv_{hash_suffix}"

if platform.system() == "Windows":
    PYTHON_BIN = VENV_PATH / "Scripts" / "python.exe"
else:
    PYTHON_BIN = VENV_PATH / "bin" / "python"

def create_and_setup_venv():
    venv.create(VENV_PATH, with_pip=True)
    subprocess.check_call([str(PYTHON_BIN), "-m", "pip", "install", "--upgrade", "pip"])
    subprocess.check_call([str(PYTHON_BIN), "-m", "pip", "install", "rich", "torch", "torchvision"])

def restart_with_venv():
    try:
        result = subprocess.run([str(PYTHON_BIN)] + sys.argv, text=True, check=True, env=dict(**os.environ))
        sys.exit(result.returncode)
    except subprocess.CalledProcessError as e:
        sys.exit(e.returncode)
    except KeyboardInterrupt:
        sys.exit(0)
    except Exception as e:
        print(f"Unexpected error while restarting python: {e}")
        sys.exit(1)

def ensure_venv_and_rich():
    try:
        import rich  # noqa: F401
        import torch  # noqa: F401
        import torchvision  # noqa: F401
    except (OSError, ModuleNotFoundError):
        if not VENV_PATH.exists():
            create_and_setup_venv()
        else:
            try:
                subprocess.check_call([str(PYTHON_BIN), "-m", "pip", "install", "-q", "rich", "torch", "torchvision"])
            except subprocess.CalledProcessError:
                shutil.rmtree(VENV_PATH)
                create_and_setup_venv()
        restart_with_venv()
    except KeyboardInterrupt:
        sys.exit(0)

ensure_venv_and_rich()

import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from rich.console import Console
from rich.progress import Progress, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn
from rich.panel import Panel
from rich.table import Table
from rich import box

console = Console()

shown_exit_msg = False

def graceful_exit(signum, frame):
    global shown_exit_msg

    if not shown_exit_msg:
        console.print("\n[bold red]You pressed Ctrl+C. Exiting gracefully.[/bold red]")
        shown_exit_msg = True

    sys.exit(0)

signal.signal(signal.SIGINT, graceful_exit)

def parse_args():
    parser = argparse.ArgumentParser(description="Train a simple neural network on CIFAR100 with CLI hyperparameters.")

    # Training hyperparameters
    parser.add_argument("--epochs", type=int, default=60, help="Number of epochs to train. (Default bumped to 60 for CIFAR100)")
    parser.add_argument("--batch_size", type=int, default=128, help="Batch size for training and testing.")
    parser.add_argument("--learning_rate", type=float, default=1e-3, help="Learning rate for optimizer.")
    parser.add_argument("--optimizer", type=str, choices=["sgd", "adam", "rmsprop"], default="adam", help="Optimizer to use.")
    parser.add_argument("--momentum", type=float, default=0.9, help="Momentum for SGD optimizer.")
    parser.add_argument("--weight_decay", type=float, default=1e-4, help="L2 weight decay (regularization).")

    # MLP / Dense layer hyperparameters
    parser.add_argument("--hidden_size", type=int, default=128, help="Number of neurons in the hidden layer.")
    parser.add_argument("--dropout", type=float, default=0.2, help="Dropout probability (0.0 = no dropout).")
    parser.add_argument("--activation", type=str, choices=["relu", "tanh", "leaky_relu", "sigmoid", "swiglu", "elu", "selu", "gelu", "softplus", "hardswish", "relu6", "mish"], default="relu", help="Activation function to use.")
    parser.add_argument("--dense_activation", type=str, choices=["relu", "tanh", "leaky_relu", "sigmoid", "swiglu", "elu", "selu", "gelu", "softplus", "hardswish", "relu6", "mish"], default="relu", help="Activation function to use for Dense-Layers.")
    parser.add_argument("--init", type=str, choices=["xavier", "kaiming", "normal", "None"], default="kaiming", help="Weight initialization method.")
    parser.add_argument("--num_dense_layers", type=int, default=2, help="Number of dense (fully-connected) layers excluding the output layer.")

    # Convolutional layer hyperparameters (neu)
    parser.add_argument("--filter", type=int, default=16, help="Number of filters per Conv2D layer")
    parser.add_argument("--num_conv_layers", type=int, default=4, help="Number of convolutional layers")

    # Conv internals (defaults better tuned)
    parser.add_argument("--conv_kernel_size", type=int, default=3, help="Conv kernel size")
    parser.add_argument("--conv_stride", type=int, default=1, help="Conv stride (use 1 and pool instead of large stride)")
    parser.add_argument("--conv_padding", type=int, default=1, help="Conv padding (for kernel 3 -> 1)")
    parser.add_argument("--pool_stride", type=int, default=2, help="Stride for pooling")
    parser.add_argument("--pool_kernel", type=int, default=2, help="Kernel for Pooling")

    # ResNet option
    parser.add_argument("--resnet", action="store_true", help="Use ResNet-style residual blocks instead of simple Conv2d layers.")
    parser.add_argument("--residual_frequency", type=int, default=1, help="After how many BasicBlocks a grouped residual addition (+ identity) should be applied (1 = every block).")

    # Misc
    parser.add_argument("--seed", type=int, default=None, help="Random seed.")
    parser.add_argument("--install", action="store_true", help="Install required dependencies (default: False)")

    return parser.parse_args()

class BasicBlock(nn.Module):
    """
    A small ResNet-like basic block with two conv layers, batchnorm and activation.
    If in_channels != out_channels, performs a 1x1 conv downsample on the residual path
    to match channels. This block does NOT change spatial resolution by itself.
    Spatial downsampling (if desired) is handled by adding a pooling layer externally
    to preserve previous script behaviour.
    """
    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1, activation_name="relu"):
        super(BasicBlock, self).__init__()
        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=padding)
        self.bn1 = nn.BatchNorm2d(out_channels)
        self.act1 = self._get_activation(activation_name)
        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=1, padding=padding)
        self.bn2 = nn.BatchNorm2d(out_channels)
        self.act2 = self._get_activation(activation_name)

        self.downsample = None
        if in_channels != out_channels or stride != 1:
            # Use 1x1 conv to match channels and (optionally) stride if requested
            self.downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )

    def _get_activation(self, name):
        name = name.lower()
        if name == "relu":
            return nn.ReLU()
        elif name == "tanh":
            return nn.Tanh()
        elif name == "sigmoid":
            return nn.Sigmoid()
        elif name == "leaky_relu":
            return nn.LeakyReLU(negative_slope=0.01)
        elif name == "swiglu":
            return nn.SiLU()
        elif name == "elu":
            return nn.ELU()
        elif name == "selu":
            return nn.SELU()
        elif name == "gelu":
            return nn.GELU()
        elif name == "softplus":
            return nn.Softplus()
        elif name == "hardswish":
            return nn.Hardswish()
        elif name == "relu6":
            return nn.ReLU6()
        else:
            raise ValueError(f"Unbekannte Aktivierung: {name}")

    def forward(self, x):
        identity = x
        out = self.conv1(x)
        out = self.bn1(out)
        out = self.act1(out)
        out = self.conv2(out)
        out = self.bn2(out)

        if self.downsample is not None:
            identity = self.downsample(identity)

        out += identity
        out = self.act2(out)
        return out

class GroupedResidualBlock(nn.Module):
    """
    N consecutive BasicBlocks with a single grouped residual addition.
    The identity is added to the output only once, after all blocks in the group.
    Downsampling / channel matching is applied to the identity if needed.
    """
    def __init__(self, in_channels, out_channels, num_blocks, kernel_size=3, stride=1, padding=1, activation_name="relu"):
        super(GroupedResidualBlock, self).__init__()
        self.blocks = nn.Sequential()
        self.downsample = None

        # If channels or stride differ, prepare a downsample for the identity path.
        if in_channels != out_channels or stride != 1:
            self.downsample = nn.Sequential(
                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride),
                nn.BatchNorm2d(out_channels)
            )

        current_in = in_channels
        for i in range(num_blocks):
            # Only the first block may carry the requested stride (for spatial reduction if desired)
            current_stride = stride if i == 0 else 1
            block = BasicBlock(current_in, out_channels,
                               kernel_size=kernel_size,
                               stride=current_stride,
                               padding=padding,
                               activation_name=activation_name)
            self.blocks.add_module(f"block{i}", block)
            current_in = out_channels

    def forward(self, x):
        identity = x
        out = self.blocks(x)
        if self.downsample is not None:
            identity = self.downsample(identity)
        out += identity
        return out

class SimpleMLP(nn.Module):
    def __init__(self,
                 input_channels,
                 input_height,
                 input_width,
                 num_classes,
                 hidden_size,
                 dropout,
                 activation,
                 dense_activation,
                 init_mode,
                 num_dense_layers,
                 num_conv_layers,
                 conv_filters,
                 conv_kernel_size=3,
                 conv_stride=1,
                 conv_padding=1,
                 pool_kernel=2,
                 pool_stride=2,
                 use_resnet=False,
                 residual_frequency=1):
        super(SimpleMLP, self).__init__()

        self.activation_fn = self.get_activation(activation)
        self.dense_activation_fn = self.get_activation(dense_activation)
        self.dropout = nn.Dropout(p=dropout)

        # Convolutional Layers â€” either simple convs (original behaviour) or ResNet blocks
        conv_layers = []
        in_channels = input_channels
        h, w = input_height, input_width

        if not use_resnet:
            # ResNet-style blocks with grouped residuals
            i = 0
            in_channels_current = in_channels
            conv_count = 0

            while i < num_conv_layers:
                group_size = min(residual_frequency, num_conv_layers - i)
                group_stride = conv_stride  # stride applied to the first BasicBlock inside the group
                block = GroupedResidualBlock(
                    in_channels_current,
                    conv_filters,
                    num_blocks=group_size,
                    kernel_size=conv_kernel_size,
                    stride=group_stride,
                    padding=conv_padding,
                    activation_name=activation
                )
                conv_layers.append((f"group{i}", block))
                in_channels_current = conv_filters

                # update spatial size for each BasicBlock inside the group
                for j in range(group_size):
                    current_stride = group_stride if j == 0 else 1
                    h = (h + 2*conv_padding - conv_kernel_size) // current_stride + 1
                    w = (w + 2*conv_padding - conv_kernel_size) // current_stride + 1
                    conv_count += 1

                    # Pool after every 2 conv layers, but only if it won't collapse spatial dims
                    if conv_count % 2 == 0 and min(h, w) > 1:
                        actual_pool_kernel = min(pool_kernel, h, w)
                        actual_pool_stride = min(pool_stride, h, w)
                        if actual_pool_kernel > 1 and actual_pool_stride > 0:
                            conv_layers.append((f"pool{conv_count//2}", nn.MaxPool2d(kernel_size=actual_pool_kernel, stride=actual_pool_stride)))
                            h = (h - actual_pool_kernel) // actual_pool_stride + 1
                            w = (w - actual_pool_kernel) // actual_pool_stride + 1

                i += group_size
        else:
            # ResNet-style blocks, but allow grouped residual frequency.
            # We'll create groups of `residual_frequency` BasicBlocks that share one residual addition.
            i = 0
            group_index = 0
            while i < num_conv_layers:
                group_size = min(residual_frequency, num_conv_layers - i)
                # Create a grouped block: first block in group uses conv_stride, others stride=1
                block = GroupedResidualBlock(in_channels, conv_filters,
                                            num_blocks=group_size,
                                            kernel_size=conv_kernel_size,
                                            stride=conv_stride,
                                            padding=conv_padding,
                                            activation_name=activation)
                conv_layers.append((f"group{group_index}", block))
                in_channels = conv_filters

                # Update spatial dimensions for each BasicBlock inside the group.
                for _ in range(group_size):
                    h = (h + 2*conv_padding - conv_kernel_size)//conv_stride + 1
                    w = (w + 2*conv_padding - conv_kernel_size)//conv_stride + 1

                # After every two original conv layers there was a pooling step.
                # The original code added pool after every two conv layers. To preserve behavior
                # we mimic the same: apply pooling after every 2 conv layers total.
                # Track i and decide whether to insert pool modules as we pass conv indices.
                # We know how many convs we've processed so far: i + group_size
                # Add pools for each completed pair since last time.
                # We'll compute how many pools should have been inserted up to this point and insert them accordingly.
                # Simpler approach: insert a pool whenever the number of conv layers processed crosses a multiple of 2.
                prev_i = i
                i += group_size
                # insert pools for each multiple of 2 crossed between prev_i and i
                pools_to_add = 0
                # count how many multiples of 2 are <= i and > prev_i
                for multiple in range(prev_i + 1, i + 1):
                    if multiple % 2 == 0:
                        pools_to_add += 1
                for p_idx in range(pools_to_add):
                    conv_layers.append((f"pool{(prev_i + p_idx + 1)//2}", nn.MaxPool2d(kernel_size=pool_kernel, stride=pool_stride)))
                    h = (h - pool_kernel) // pool_stride + 1
                    w = (w - pool_kernel) // pool_stride + 1

                group_index += 1

        # convert list of tuples into Sequential with ordered names
        conv_seq = nn.Sequential()
        for name, module in conv_layers:
            conv_seq.add_module(name, module)

        self.conv = conv_seq
        flattened_size = in_channels * max(1, h) * max(1, w)

        # Dense Layers
        dense_layers = nn.Sequential()
        in_features = flattened_size
        for i in range(num_dense_layers):
            dense_layers.add_module(f"fc{i}", nn.Linear(in_features, hidden_size))
            dense_layers.add_module(f"act{i}", self.get_activation(dense_activation))
            dense_layers.add_module(f"dropout{i}", nn.Dropout(p=dropout))
            in_features = hidden_size

        dense_layers.add_module("output", nn.Linear(in_features, num_classes))

        self.classifier = dense_layers

        # Initialization
        if init_mode is not None and init_mode != "None":
            self.init_weights(init_mode)

    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)

    def get_activation(self, name):
        name = name.lower()
        if name == "relu":
            return nn.ReLU()
        elif name == "tanh":
            return nn.Tanh()
        elif name == "sigmoid":
            return nn.Sigmoid()
        elif name == "leaky_relu":
            return nn.LeakyReLU(negative_slope=0.01)
        elif name == "swiglu":
            return nn.SiLU()
        elif name == "elu":
            return nn.ELU()
        elif name == "selu":
            return nn.SELU()
        elif name == "gelu":
            return nn.GELU()
        elif name == "softplus":
            return nn.Softplus()
        elif name == "hardswish":
            return nn.Hardswish()
        elif name == "relu6":
            return nn.ReLU6()
        else:
            raise ValueError(f"Unbekannte Aktivierung: {name}")

    def init_weights(self, mode):
        for m in self.modules():
            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):
                if mode == "xavier":
                    nn.init.xavier_uniform_(m.weight)
                elif mode == "kaiming":
                    # Kaiming for ReLU-style activations
                    nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')
                elif mode == "normal":
                    nn.init.normal_(m.weight, mean=0.0, std=0.02)
                else:
                    raise ValueError(f"Unbekannte Init-Methode: {mode}")
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

def show_hyperparams(args):
    table = Table(title="Hyperparameters", box=box.ROUNDED, title_style="bold magenta")
    table.add_column("Parameter", style="bold cyan")
    table.add_column("Value", style="bold green")

    table.add_row("Epochs", str(args.epochs))
    table.add_row("Num Dense Layers", str(args.num_dense_layers))
    table.add_row("Batch size", str(args.batch_size))
    table.add_row("Learning rate", str(args.learning_rate))
    table.add_row("Hidden size", str(args.hidden_size))
    table.add_row("Dropout", str(args.dropout))
    table.add_row("Optimizer", args.optimizer)
    table.add_row("Momentum", str(args.momentum))
    table.add_row("Weight Decay", str(args.weight_decay))
    table.add_row("Activation", args.activation)
    table.add_row("Dense-Activation", args.dense_activation)
    table.add_row("Init Method", args.init)
    table.add_row("Seed", str(args.seed) if args.seed is not None else "None")
    table.add_row("Conv Filters", str(args.filter))
    table.add_row("Num Conv Layers", str(args.num_conv_layers))
    table.add_row("Conv Kernel", str(args.conv_kernel_size))
    table.add_row("Conv Stride", str(args.conv_stride))
    table.add_row("Conv Padding", str(args.conv_padding))
    table.add_row("Pool-Stride", str(args.pool_stride))
    table.add_row("Pool-Kernel", str(args.pool_kernel))
    table.add_row("Use ResNet Blocks", str(getattr(args, "resnet", False)))
    table.add_row("Residual Frequency", str(getattr(args, "residual_frequency", 1)))

    console.print(table)

def print_model_summary(model, input_size):
    device = next(model.parameters()).device
    x = torch.zeros(1, *input_size).to(device)

    table = Table(
        title="Model Summary",
        box=box.ROUNDED,
        title_style="bold magenta"
    )
    table.add_column("Layer", style="bold cyan")
    table.add_column("Output Shape", style="bold green")
    table.add_column("Param #", justify="right", style="bold yellow")

    total_params = 0

    # iterate through conv submodules in order
    for name, layer in model.conv._modules.items():
        x = layer(x)
        params = sum(p.numel() for p in layer.parameters() if p.requires_grad)
        total_params += params
        table.add_row(f"conv::{name}", str(list(x.shape)), str(params))

    # Flatten for dense layers
    x = x.view(x.size(0), -1)

    # iterate dense
    for name, layer in model.classifier._modules.items():
        x = layer(x)
        params = sum(p.numel() for p in layer.parameters() if p.requires_grad)
        total_params += params
        out_shape_str = str(list(x.shape))
        table.add_row(f"dense::{name}", out_shape_str, str(params))

    # Gesamtparameter
    table.add_row("Total", "-", str(total_params))

    console.print(table)

def train(model, device, train_loader, criterion, optimizer, epoch, total_epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    console.rule(f"[bold blue]Epoch {epoch}/{total_epochs} - Training")

    with Progress(
        TextColumn("[bold blue]Batch {task.completed}/{task.total}"),
        BarColumn(),
        TaskProgressColumn(),
        TimeRemainingColumn(),
        TextColumn("Loss: [green]{task.fields[loss]:.4f}"),
        TextColumn("Acc: [cyan]{task.fields[acc]:.2f}%"),
        transient=True,
    ) as progress:
        task_id = progress.add_task("Training", total=len(train_loader), loss=0.0, acc=0.0)

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

            progress.update(
                task_id,
                advance=1,
                loss=running_loss / (batch_idx + 1),
                acc=100.0 * correct / total
            )

    console.print(f"[green]Epoch-Loss[/green]: {running_loss:.4f}")

def test(model, device, test_loader, criterion, epoch, total_epochs):
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0

    console.rule(f"[bold magenta]Epoch {epoch}/{total_epochs} - Validation")

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)
            test_loss += loss.item()

            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    if len(test_loader) > 0:
        test_loss /= len(test_loader)
    accuracy = 100.0 * correct / total if total > 0 else 0.0

    console.print(
        Panel.fit(
            f"[bold green]Validation Loss:[/] {test_loss:.4f}\n"
            f"[bold green]Accuracy:[/] {accuracy:.2f}%",
            title=f"Epoch {epoch}/{total_epochs} Summary",
            box=box.DOUBLE
        )
    )

    return test_loss, accuracy

def main():
    try:
        args = parse_args()

        if args.seed is not None:
            torch.manual_seed(args.seed)
            torch.cuda.manual_seed_all(args.seed)

        # device + speed tweaks
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if torch.backends.cudnn.is_available():
            torch.backends.cudnn.benchmark = True

        # Transforms: TRAIN (with augmentation) vs TEST (deterministic)
        train_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(
                (0.5071, 0.4867, 0.4408),
                (0.2675, 0.2565, 0.2761)
            )
        ])

        test_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(
                (0.5071, 0.4867, 0.4408),
                (0.2675, 0.2565, 0.2761)
            )
        ])

        script_dir = os.path.dirname(__file__)
        data_dir = os.path.join(script_dir, "cifar")

        # download datasets
        train_dataset = datasets.CIFAR100(root=data_dir, train=True, download=True, transform=train_transform)
        test_dataset = datasets.CIFAR100(root=data_dir, train=False, download=True, transform=test_transform)

        if args.install:
            print("Exiting, since the installation should now be done")
            sys.exit(0)

        show_hyperparams(args)

        # DataLoader performance options
        num_workers = min(4, (os.cpu_count() or 1))
        pin_memory = True if torch.cuda.is_available() else False

        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)

        model = SimpleMLP(
            input_height=32,
            input_width=32,
            input_channels=3,
            hidden_size=args.hidden_size,
            num_classes=100,
            dropout=args.dropout,
            activation=args.activation,
            dense_activation=args.dense_activation,
            init_mode=args.init,
            num_dense_layers=args.num_dense_layers,
            num_conv_layers=args.num_conv_layers,
            conv_filters=args.filter,
            conv_kernel_size=args.conv_kernel_size,
            conv_stride=args.conv_stride,
            conv_padding=args.conv_padding,
            pool_kernel=args.pool_kernel,
            pool_stride=args.pool_stride,
            use_resnet=args.resnet,
            residual_frequency=args.residual_frequency,
        ).to(device)

        print_model_summary(model, input_size=(3, 32, 32))

        if args.learning_rate == 0:
            console.print("[red]Learning Rate cannot be 0[/red]")
            sys.exit(1)

        if args.optimizer == "adam":
            optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
        elif args.optimizer == "sgd":
            optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)
        elif args.optimizer == "rmsprop":
            optimizer = torch.optim.RMSprop(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
        else:
            raise ValueError(f"Unknown optimizer: {args.optimizer}")

        # Scheduler: StepLR is simple and effective; you can swap to ReduceLROnPlateau if desired.
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

        criterion = nn.CrossEntropyLoss()

        best_acc = 0.0
        best_loss = float("inf")
        for epoch in range(1, args.epochs + 1):
            train(model, device, train_loader, criterion, optimizer, epoch, args.epochs)
            val_loss, val_acc = test(model, device, test_loader, criterion, epoch, args.epochs)

            # step scheduler (for StepLR)
            scheduler.step()

            # save checkpoint if improved (optional)
            #if val_acc > best_acc:
            #    best_acc = val_acc
            #    try:
            #        ckpt_path = os.path.join(script_dir, "best_model.pth")
            #        torch.save({"epoch": epoch, "model_state": model.state_dict(), "optimizer_state": optimizer.state_dict()}, ckpt_path)
            #        console.print(f"[green]Saved improved model to {ckpt_path} (acc {best_acc:.2f}%)[/green]")
            #    except Exception as e:
            #        console.print(f"[red]Failed to save checkpoint: {e}[/red]")

            best_loss = min(best_loss, val_loss)

        print(f"VAL_LOSS: {val_loss}")
        print(f"VAL_ACC: {val_acc}")

        end_time = time.time()
        runtime_seconds = end_time - start_time

        print(f"RUNTIME: {runtime_seconds:.3f}")
        max_runtime = 7200
        normalized_runtime = (runtime_seconds / max_runtime) * 100
        normalized_runtime = round(normalized_runtime, 3)
        print(f"NORMALIZED_RUNTIME: {normalized_runtime}")

    except KeyboardInterrupt:
        graceful_exit(None, None)
    except Exception as e:
        console.print(f"[bold red]An error occurred: {e}[/bold red]")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
