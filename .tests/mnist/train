#!/usr/bin/env python3

import sys
import os
import platform
import shutil
import subprocess
import signal
import traceback
from pathlib import Path
import hashlib
import time

start_time = time.time()

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

SCRIPT_DIR = Path(__file__).resolve().parent

try:
    import venv
except ModuleNotFoundError:
    print("venv not found. Is python3-venv installed?")
    sys.exit(1)

clusterhost = os.environ.get("CLUSTERHOST", "").encode()
hash_suffix = hashlib.sha256(clusterhost).hexdigest()[:10]
VENV_PATH = SCRIPT_DIR / f".torch_venv_{hash_suffix}"

if platform.system() == "Windows":
    PYTHON_BIN = VENV_PATH / "Scripts" / "python.exe"
else:
    PYTHON_BIN = VENV_PATH / "bin" / "python"

def create_and_setup_venv():
    venv.create(VENV_PATH, with_pip=True)
    subprocess.check_call([str(PYTHON_BIN), "-m", "pip", "install", "--upgrade", "pip"])
    subprocess.check_call([str(PYTHON_BIN), "-m", "pip", "install", "rich", "torch", "torchvision"])

def restart_with_venv():
    try:
        result = subprocess.run([str(PYTHON_BIN)] + sys.argv, text=True, check=True, env=dict(**os.environ))
        sys.exit(result.returncode)
    except subprocess.CalledProcessError as e:
        sys.exit(e.returncode)
    except KeyboardInterrupt:
        sys.exit(0)
    except Exception as e:
        print(f"Unexpected error while restarting python: {e}")
        sys.exit(1)

def ensure_venv_and_rich():
    try:
        import rich
        import torch
        import torchvision
    except (OSError, ModuleNotFoundError):
        if not VENV_PATH.exists():
            create_and_setup_venv()
        else:
            try:
                subprocess.check_call([str(PYTHON_BIN), "-m", "pip", "install", "-q", "rich", "torch", "torchvision"])
            except subprocess.CalledProcessError:
                shutil.rmtree(VENV_PATH)
                create_and_setup_venv()
        restart_with_venv()
    except KeyboardInterrupt:
        sys.exit(0)

ensure_venv_and_rich()

import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from rich.console import Console
from rich.progress import Progress, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn
from rich.panel import Panel
from rich.table import Table
from rich import box

console = Console()

def graceful_exit(signum, frame):
    console.print("\n[bold red]You pressed Ctrl+C. Exiting gracefully.[/bold red]")
    sys.exit(0)

signal.signal(signal.SIGINT, graceful_exit)

def parse_args():
    parser = argparse.ArgumentParser(description="Train a simple neural network on CIFAR100 with CLI hyperparameters.")

    # Training hyperparameters
    parser.add_argument("--epochs", type=int, default=10, help="Number of epochs to train.")
    parser.add_argument("--batch_size", type=int, default=64, help="Batch size for training and testing.")
    parser.add_argument("--learning_rate", type=float, default=0.0005, help="Learning rate for optimizer.")
    parser.add_argument("--optimizer", type=str, choices=["sgd", "adam", "rmsprop"], default="adam", help="Optimizer to use.")
    parser.add_argument("--momentum", type=float, default=0.9, help="Momentum for SGD optimizer.")
    parser.add_argument("--weight_decay", type=float, default=0.0, help="L2 weight decay (regularization).")

    # MLP / Dense layer hyperparameters
    parser.add_argument("--hidden_size", type=int, default=256, help="Number of neurons in the hidden layer.")
    parser.add_argument("--dropout", type=float, default=0.0, help="Dropout probability (0.0 = no dropout).")
    parser.add_argument("--activation", type=str, choices=["relu", "tanh", "leaky_relu", "sigmoid"], default="relu", help="Activation function to use.")
    parser.add_argument("--init", type=str, choices=["xavier", "kaiming", "normal", "None"], default="None", help="Weight initialization method.")
    parser.add_argument("--num_dense_layers", type=int, default=1, help="Number of dense (fully-connected) layers excluding the output layer.")

    # Convolutional layer hyperparameters (neu)
    parser.add_argument("--filter", type=int, default=64, help="Number of filters per Conv2D layer")
    parser.add_argument("--num_conv_layers", type=int, default=3, help="Number of convolutional layers")

    # Misc
    parser.add_argument("--seed", type=int, default=None, help="Random seed.")
    parser.add_argument("--install", action="store_true", help="Install required dependencies (default: False)")

    return parser.parse_args()

class SimpleMLP(nn.Module):
    def __init__(self,
                 input_channels,
                 input_height,
                 input_width,
                 num_classes,
                 hidden_size,
                 dropout,
                 activation,
                 init_mode,
                 num_dense_layers,
                 num_conv_layers,
                 conv_filters,
                 conv_kernel_size=3,
                 conv_stride=2,
                 conv_padding=2):
        super(SimpleMLP, self).__init__()

        self.activation_fn = self.get_activation(activation)
        self.dropout = nn.Dropout(p=dropout)

        # Convolutional Layers
        conv_layers = nn.Sequential()
        in_channels = input_channels
        h, w = input_height, input_width

        for i in range(num_conv_layers):
            conv_layers.add_module(f"conv{i}", nn.Conv2d(in_channels, conv_filters,
                                                         kernel_size=conv_kernel_size,
                                                         stride=conv_stride,
                                                         padding=conv_padding))

            conv_layers.add_module(f"bn{i}",nn.BatchNorm2d(conv_filters))

            conv_layers.add_module(f"act_conv{i}", self.activation_fn)

            in_channels = conv_filters
            h = (h + 2*conv_padding - conv_kernel_size)//conv_stride + 1
            w = (w + 2*conv_padding - conv_kernel_size)//conv_stride + 1

        self.conv = conv_layers
        flattened_size = in_channels * h * w

        # Dense Layers
        dense_layers = nn.Sequential()
        in_features = flattened_size
        for i in range(num_dense_layers):
            dense_layers.add_module(f"fc{i}", nn.Linear(in_features, hidden_size))
            dense_layers.add_module(f"act{i}", self.activation_fn)
            dense_layers.add_module(f"dropout{i}", self.dropout)
            in_features = hidden_size

        dense_layers.add_module("output", nn.Linear(in_features, num_classes))
        #dense_layers.add_module("softmax", nn.Softmax(dim=1))

        self.classifier = dense_layers

        # Initialisierung
        if init_mode is not None and init_mode != "None":
            self.init_weights(init_mode)

    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)

    def get_activation(self, name):
        name = name.lower()
        if name == "relu":
            return nn.ReLU()
        elif name == "tanh":
            return nn.Tanh()
        elif name == "sigmoid":
            return nn.Sigmoid()
        elif name == "leaky_relu":
            return nn.LeakyReLU()
        else:
            raise ValueError(f"Unbekannte Aktivierung: {name}")

    def init_weights(self, mode):
        for m in self.modules():
            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):
                if mode == "xavier":
                    nn.init.xavier_uniform_(m.weight)
                elif mode == "kaiming":
                    nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')
                elif mode == "normal":
                    nn.init.normal_(m.weight, mean=0.0, std=0.02)
                else:
                    raise ValueError(f"Unbekannte Init-Methode: {mode}")
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

def show_hyperparams(args):
    table = Table(title="Hyperparameters", box=box.ROUNDED, title_style="bold magenta")
    table.add_column("Parameter", style="bold cyan")
    table.add_column("Value", style="bold green")

    table.add_row("Epochs", str(args.epochs))
    table.add_row("Num Dense Layers", str(args.num_dense_layers))
    table.add_row("Batch size", str(args.batch_size))
    table.add_row("Learning rate", str(args.learning_rate))
    table.add_row("Hidden size", str(args.hidden_size))
    table.add_row("Dropout", str(args.dropout))
    table.add_row("Optimizer", args.optimizer)
    table.add_row("Momentum", str(args.momentum))
    table.add_row("Weight Decay", str(args.weight_decay))
    table.add_row("Activation", args.activation)
    table.add_row("Init Method", args.init)
    table.add_row("Seed", str(args.seed) if args.seed is not None else "None")
    table.add_row("Conv Filters", str(args.filter))
    table.add_row("Num Conv Layers", str(args.num_conv_layers))

    console.print(table)

# --- train, test, print_model_summary wie bisher ---
# Ich lasse die restlichen Funktionen unverändert, da nur die Conv-Layer-Logik angepasst wurde.

def print_model_summary(model, input_size):
    device = next(model.parameters()).device
    x = torch.zeros(1, *input_size).to(device)

    table = Table(
        title="Model Summary",
        box=box.ROUNDED,
        title_style="bold magenta"
    )
    table.add_column("Layer", style="bold cyan")
    table.add_column("Output Shape", style="bold green")
    table.add_column("Param #", justify="right", style="bold yellow")

    total_params = 0

    # Iteriere zuerst über die Conv-Schichten
    for name, layer in model.conv._modules.items():
        x = layer(x)
        params = sum(p.numel() for p in layer.parameters() if p.requires_grad)
        total_params += params
        table.add_row(f"conv::{name}", str(list(x.shape)), str(params))

    # Flatten für Dense-Layer
    x = x.view(x.size(0), -1)

    # Iteriere über Dense-Layer
    for name, layer in model.classifier._modules.items():
        x = layer(x)
        params = sum(p.numel() for p in layer.parameters() if p.requires_grad)
        total_params += params
        out_shape_str = str(list(x.shape))
        table.add_row(f"dense::{name}", out_shape_str, str(params))

    # Gesamtparameter
    table.add_row("Total", "-", str(total_params))

    console.print(table)

def train(model, device, train_loader, criterion, optimizer, epoch, total_epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    console.rule(f"[bold blue]Epoch {epoch}/{total_epochs} - Training")

    with Progress(
        TextColumn("[bold blue]Batch {task.completed}/{task.total}"),
        BarColumn(),
        TaskProgressColumn(),
        TimeRemainingColumn(),
        TextColumn("Loss: [green]{task.fields[loss]:.4f}"),
        TextColumn("Acc: [cyan]{task.fields[acc]:.2f}%"),
        transient=True,
    ) as progress:
        task_id = progress.add_task("Training", total=len(train_loader), loss=0.0, acc=0.0)

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

            progress.update(
                task_id,
                advance=1,
                loss=running_loss / (batch_idx + 1),
                acc=100.0 * correct / total
            )

    console.print(f"[green]Epoch-Loss[/green]: {running_loss:.4f}")


def test(model, device, test_loader, criterion, epoch, total_epochs):
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0

    console.rule(f"[bold magenta]Epoch {epoch}/{total_epochs} - Validation")

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)
            test_loss += loss.item()

            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    test_loss /= len(test_loader)
    accuracy = 100.0 * correct / total

    console.print(
        Panel.fit(
            f"[bold green]Validation Loss:[/] {test_loss:.4f}\n"
            f"[bold green]Accuracy:[/] {accuracy:.2f}%",
            title=f"Epoch {epoch}/{total_epochs} Summary",
            box=box.DOUBLE
        )
    )

    return test_loss, accuracy

def main():
    try:
        args = parse_args()

        show_hyperparams(args)

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

        # for mnist:
        #transform = transforms.Compose([
        #    transforms.ToTensor(),
        #    transforms.Normalize((0.1307,), (0.3081,))
        #])

        # for CIFAR100
        transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(
                (0.5071, 0.4867, 0.4408),
                (0.2675, 0.2565, 0.2761)
            )
        ])

        script_dir = os.path.dirname(__file__)
        data_dir = os.path.join(script_dir, "cifar")

        train_dataset = datasets.CIFAR100(root=data_dir, train=True, download=True, transform=transform)
        test_dataset = datasets.CIFAR100(root=data_dir, train=False, download=True, transform=transform)

        if args.install:
            print("Exiting, since the installation should now be done")
            sys.exit(0)

        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False)

        if args.seed is not None:
            torch.manual_seed(args.seed)

        model = SimpleMLP(
            input_height=32,
            input_width=32,
            input_channels=3,
            hidden_size=args.hidden_size,
            num_classes=100,
            dropout=args.dropout,
            activation=args.activation,
            init_mode=args.init,
            num_dense_layers=args.num_dense_layers,
            num_conv_layers=args.num_conv_layers,
            conv_filters=args.filter,
            conv_kernel_size=3,
            conv_stride=2,
            conv_padding=2
        ).to(device)

        print_model_summary(model, input_size=(3, 32, 32))

        if args.learning_rate == 0:
            console.print("[red]Learning Rate cannot be 0[/red]")
            sys.exit(1)

        if args.optimizer == "adam":
            optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
        elif args.optimizer == "sgd":
            optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)
        elif args.optimizer == "rmsprop":
            optimizer = torch.optim.RMSprop(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
        else:
            raise ValueError(f"Unknown optimizer: {args.optimizer}")

        criterion = nn.CrossEntropyLoss()

        for epoch in range(1, args.epochs + 1):
            train(model, device, train_loader, criterion, optimizer, epoch, args.epochs)
            val_loss, val_acc = test(model, device, test_loader, criterion, epoch, args.epochs)

        print(f"VAL_LOSS: {val_loss}")
        print(f"VAL_ACC: {val_acc}")

        end_time = time.time()
        runtime_seconds = end_time - start_time

        print(f"RUNTIME: {runtime_seconds:.3f}")
        max_runtime = 7200
        normalized_runtime = (runtime_seconds / max_runtime) * 100
        normalized_runtime = round(normalized_runtime, 3)
        print(f"NORMALIZED_RUNTIME: {normalized_runtime}")

    except KeyboardInterrupt:
        graceful_exit(None, None)
    except Exception as e:
        console.print(f"[bold red]An error occurred: {e}[/bold red]", style="bold red")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
