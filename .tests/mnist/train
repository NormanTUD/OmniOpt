#!/usr/bin/env python3

import sys
import os
import platform
import shutil
import subprocess
import signal
import traceback
from pathlib import Path
import hashlib
import time
import math
import random

start_time = time.time()

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

SCRIPT_DIR = Path(__file__).resolve().parent

try:
    import venv
except ModuleNotFoundError:
    print("venv not found. Is python3-venv installed?")
    sys.exit(1)

clusterhost = os.environ.get("CLUSTERHOST", "").encode()
hash_suffix = hashlib.sha256(clusterhost).hexdigest()[:10]
VENV_PATH = SCRIPT_DIR / f".torch_venv_{hash_suffix}"

if platform.system() == "Windows":
    PYTHON_BIN = VENV_PATH / "Scripts" / "python.exe"
else:
    PYTHON_BIN = VENV_PATH / "bin" / "python"

def create_and_setup_venv():
    venv.create(VENV_PATH, with_pip=True)
    subprocess.check_call([str(PYTHON_BIN), "-m", "pip", "install", "--upgrade", "pip"])
    subprocess.check_call([str(PYTHON_BIN), "-m", "pip", "install", "rich", "torch", "torchvision"])

def restart_with_venv():
    try:
        result = subprocess.run([str(PYTHON_BIN)] + sys.argv, text=True, check=True, env=dict(**os.environ))
        sys.exit(result.returncode)
    except subprocess.CalledProcessError as e:
        sys.exit(e.returncode)
    except KeyboardInterrupt:
        sys.exit(0)
    except Exception as e:
        print(f"Unexpected error while restarting python: {e}")
        sys.exit(1)

def ensure_venv_and_rich():
    try:
        import rich  # noqa: F401
        import torch  # noqa: F401
        import torchvision  # noqa: F401
    except (OSError, ModuleNotFoundError):
        if not VENV_PATH.exists():
            create_and_setup_venv()
        else:
            try:
                subprocess.check_call([str(PYTHON_BIN), "-m", "pip", "install", "-q", "rich", "torch", "torchvision"])
            except subprocess.CalledProcessError:
                shutil.rmtree(VENV_PATH)
                create_and_setup_venv()
        restart_with_venv()
    except KeyboardInterrupt:
        sys.exit(0)

ensure_venv_and_rich()

import argparse
import numpy as np
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms, models
from rich.console import Console
from rich.progress import Progress, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn
from rich.panel import Panel
from rich.table import Table
from rich import box

console = Console()

def graceful_exit(signum, frame):
    console.print("\n[bold red]You pressed Ctrl+C. Exiting gracefully.[/bold red]")
    sys.exit(0)

signal.signal(signal.SIGINT, graceful_exit)

def parse_args():
    parser = argparse.ArgumentParser(description="Train a neural network on CIFAR100 with improved defaults.")
    parser.add_argument("--epochs", type=int, default=60, help="Number of epochs to train.")
    parser.add_argument("--batch_size", type=int, default=128, help="Batch size for training and testing.")
    parser.add_argument("--learning_rate", type=float, default=0.1, help="Learning rate for optimizer (SGD default uses 0.1).")
    parser.add_argument("--optimizer", type=str, choices=["sgd", "adam", "rmsprop"], default="sgd", help="Optimizer to use.")
    parser.add_argument("--momentum", type=float, default=0.9, help="Momentum for SGD optimizer.")
    parser.add_argument("--weight_decay", type=float, default=5e-4, help="L2 weight decay (regularization).")
    parser.add_argument("--hidden_size", type=int, default=512, help="Hidden size for simple MLP (if used).")
    parser.add_argument("--dropout", type=float, default=0.2, help="Dropout probability.")
    parser.add_argument("--activation", type=str, choices=["relu", "tanh", "leaky_relu", "sigmoid"], default="relu", help="Activation function.")
    parser.add_argument("--init", type=str, choices=["xavier", "kaiming", "normal", "None"], default="kaiming", help="Weight initialization.")
    parser.add_argument("--num_dense_layers", type=int, default=2, help="Number of dense layers (simple MLP).")
    parser.add_argument("--filter", type=int, default=64, help="Conv filters for simple MLP.")
    parser.add_argument("--num_conv_layers", type=int, default=4, help="Conv layers for simple MLP.")
    parser.add_argument("--seed", type=int, default=None, help="Random seed.")
    parser.add_argument("--install", action="store_true", help="Install required dependencies and exit.")
    parser.add_argument("--arch", type=str, choices=["resnet", "simple"], default="resnet", help="Architecture to train. Default: resnet (adapted ResNet-18 for CIFAR).")
    parser.add_argument("--amp", action="store_true", help="Use automatic mixed precision (fp16) training where supported.")
    parser.add_argument("--label_smoothing", type=float, default=0.1, help="Label smoothing factor for CrossEntropyLoss. 0 disables.")
    return parser.parse_args()

class SimpleMLP(nn.Module):
    def __init__(self,
                 input_channels,
                 input_height,
                 input_width,
                 num_classes,
                 hidden_size,
                 dropout,
                 activation,
                 init_mode,
                 num_dense_layers,
                 num_conv_layers,
                 conv_filters,
                 conv_kernel_size=3,
                 conv_stride=1,
                 conv_padding=1):
        super(SimpleMLP, self).__init__()

        self.activation_fn = self.get_activation(activation)
        self.dropout = nn.Dropout(p=dropout)

        conv_layers = []
        in_channels = input_channels
        h, w = input_height, input_width

        for i in range(num_conv_layers):
            conv_layers.append((f"conv{i}", nn.Conv2d(in_channels, conv_filters,
                                                      kernel_size=conv_kernel_size,
                                                      stride=conv_stride,
                                                      padding=conv_padding)))
            conv_layers.append((f"bn{i}", nn.BatchNorm2d(conv_filters)))
            conv_layers.append((f"act_conv{i}", self.get_activation(activation)))
            in_channels = conv_filters
            h = (h + 2*conv_padding - conv_kernel_size)//conv_stride + 1
            w = (w + 2*conv_padding - conv_kernel_size)//conv_stride + 1
            if (i + 1) % 2 == 0:
                conv_layers.append((f"pool{(i+1)//2}", nn.MaxPool2d(kernel_size=2, stride=2)))
                h = h // 2
                w = w // 2

        conv_seq = nn.Sequential()
        for name, module in conv_layers:
            conv_seq.add_module(name, module)

        self.conv = conv_seq
        flattened_size = in_channels * max(1, h) * max(1, w)

        dense_layers = nn.Sequential()
        in_features = flattened_size
        for i in range(num_dense_layers):
            dense_layers.add_module(f"fc{i}", nn.Linear(in_features, hidden_size))
            dense_layers.add_module(f"act{i}", self.get_activation(activation))
            dense_layers.add_module(f"dropout{i}", nn.Dropout(p=dropout))
            in_features = hidden_size

        dense_layers.add_module("output", nn.Linear(in_features, num_classes))
        self.classifier = dense_layers

        if init_mode is not None and init_mode != "None":
            self.init_weights(init_mode)

    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)

    def get_activation(self, name):
        name = name.lower()
        if name == "relu":
            return nn.ReLU()
        elif name == "tanh":
            return nn.Tanh()
        elif name == "sigmoid":
            return nn.Sigmoid()
        elif name == "leaky_relu":
            return nn.LeakyReLU(negative_slope=0.01)
        else:
            raise ValueError(f"Unknown activation: {name}")

    def init_weights(self, mode):
        for m in self.modules():
            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):
                if mode == "xavier":
                    nn.init.xavier_uniform_(m.weight)
                elif mode == "kaiming":
                    nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')
                elif mode == "normal":
                    nn.init.normal_(m.weight, mean=0.0, std=0.02)
                else:
                    raise ValueError(f"Unknown init: {mode}")
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

def build_resnet18_cifar(num_classes=100, pretrained=False):
    # Load standard ResNet18 and adapt first conv + maxpool for CIFAR (32x32)
    model = models.resnet18(pretrained=pretrained)
    # Replace conv1: 7x7 stride2 not suitable for 32x32 -> 3x3, stride1, padding1
    model.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)
    # Remove the first maxpool to preserve spatial resolution
    model.maxpool = nn.Identity()
    # Adapt final fc
    model.fc = nn.Linear(model.fc.in_features, num_classes)
    return model

def show_hyperparams(args):
    table = Table(title="Hyperparameters", box=box.ROUNDED, title_style="bold magenta")
    table.add_column("Parameter", style="bold cyan")
    table.add_column("Value", style="bold green")

    table.add_row("Epochs", str(args.epochs))
    table.add_row("Architecture", args.arch)
    table.add_row("Batch size", str(args.batch_size))
    table.add_row("Learning rate", str(args.learning_rate))
    table.add_row("Optimizer", args.optimizer)
    table.add_row("Momentum", str(args.momentum))
    table.add_row("Weight Decay", str(args.weight_decay))
    table.add_row("Hidden size (MLP)", str(args.hidden_size))
    table.add_row("Dropout", str(args.dropout))
    table.add_row("Label smoothing", str(args.label_smoothing))
    table.add_row("AMP", str(args.amp))
    table.add_row("Seed", str(args.seed) if args.seed is not None else "None")

    console.print(table)

def print_model_summary_generic(model, input_size):
    device = next(model.parameters()).device
    x = torch.zeros(1, *input_size).to(device)
    try:
        with torch.no_grad():
            y = model(x)
    except Exception:
        y = None

    total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    table = Table(title="Model Summary", box=box.ROUNDED, title_style="bold magenta")
    table.add_column("Item", style="bold cyan")
    table.add_column("Value", style="bold green")

    table.add_row("Model Type", model.__class__.__name__)
    table.add_row("Total trainable params", str(total_params))
    if y is not None:
        table.add_row("Example output shape", str(list(y.shape)))
    console.print(table)

def train_one_epoch(model, device, train_loader, criterion, optimizer, epoch, total_epochs, scaler=None, amp=False):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    console.rule(f"[bold blue]Epoch {epoch}/{total_epochs} - Training")

    with Progress(
        TextColumn("[bold blue]Batch {task.completed}/{task.total}"),
        BarColumn(),
        TaskProgressColumn(),
        TimeRemainingColumn(),
        TextColumn("Loss: [green]{task.fields[loss]:.4f}"),
        TextColumn("Acc: [cyan]{task.fields[acc]:.2f}%"),
        transient=True,
    ) as progress:
        task_id = progress.add_task("Training", total=len(train_loader), loss=0.0, acc=0.0)

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            if amp and scaler is not None:
                with torch.cuda.amp.autocast():
                    output = model(data)
                    loss = criterion(output, target)
                scaler.scale(loss).backward()
                scaler.step(optimizer)
                scaler.update()
            else:
                output = model(data)
                loss = criterion(output, target)
                loss.backward()
                optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

            progress.update(
                task_id,
                advance=1,
                loss=running_loss / (batch_idx + 1),
                acc=100.0 * correct / total
            )

    console.print(f"[green]Epoch-Loss[/green]: {running_loss:.4f}")

def validate(model, device, test_loader, criterion, epoch, total_epochs):
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0

    console.rule(f"[bold magenta]Epoch {epoch}/{total_epochs} - Validation")

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)
            test_loss += loss.item()

            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    if len(test_loader) > 0:
        test_loss /= len(test_loader)
    accuracy = 100.0 * correct / total if total > 0 else 0.0

    console.print(
        Panel.fit(
            f"[bold green]Validation Loss:[/] {test_loss:.4f}\n"
            f"[bold green]Accuracy:[/] {accuracy:.2f}%",
            title=f"Epoch {epoch}/{total_epochs} Summary",
            box=box.DOUBLE
        )
    )

    return test_loss, accuracy

def set_random_seeds(seed):
    if seed is None:
        return
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)

def main():
    try:
        args = parse_args()

        if args.install:
            print("Exiting after installing dependencies (if requested).")
            sys.exit(0)

        show_hyperparams(args)

        set_random_seeds(args.seed)

        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if torch.backends.cudnn.is_available():
            torch.backends.cudnn.benchmark = True

        # Transforms
        train_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.RandomRotation(15),
            transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),
            transforms.ToTensor(),
            transforms.Normalize((0.5071, 0.4867, 0.4408),
                                 (0.2675, 0.2565, 0.2761))
        ])

        test_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5071, 0.4867, 0.4408),
                                 (0.2675, 0.2565, 0.2761))
        ])

        script_dir = os.path.dirname(__file__)
        data_dir = os.path.join(script_dir, "cifar")

        train_dataset = datasets.CIFAR100(root=data_dir, train=True, download=True, transform=train_transform)
        test_dataset = datasets.CIFAR100(root=data_dir, train=False, download=True, transform=test_transform)

        num_workers = min(4, (os.cpu_count() or 1))
        pin_memory = True if torch.cuda.is_available() else False

        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)

        # Build model
        if args.arch == "resnet":
            model = build_resnet18_cifar(num_classes=100, pretrained=False).to(device)
        else:
            model = SimpleMLP(
                input_height=32,
                input_width=32,
                input_channels=3,
                hidden_size=args.hidden_size,
                num_classes=100,
                dropout=args.dropout,
                activation=args.activation,
                init_mode=args.init,
                num_dense_layers=args.num_dense_layers,
                num_conv_layers=args.num_conv_layers,
                conv_filters=args.filter,
                conv_kernel_size=3,
                conv_stride=1,
                conv_padding=1
            ).to(device)

        print_model_summary_generic(model, input_size=(3, 32, 32))

        # Loss: label smoothing optional
        if hasattr(nn.CrossEntropyLoss, "__call__"):
            try:
                criterion = nn.CrossEntropyLoss(label_smoothing=args.label_smoothing if args.label_smoothing > 0 else 0.0)
            except TypeError:
                # older PyTorch without label_smoothing
                criterion = nn.CrossEntropyLoss()
        else:
            criterion = nn.CrossEntropyLoss()

        # Optimizer selection
        if args.optimizer == "sgd":
            optimizer = optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)
        elif args.optimizer == "adam":
            optimizer = optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
        elif args.optimizer == "rmsprop":
            optimizer = optim.RMSprop(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
        else:
            raise ValueError(f"Unknown optimizer: {args.optimizer}")

        # Scheduler
        try:
            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=args.epochs)
        except Exception:
            scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

        # AMP scaler if requested and available
        scaler = torch.cuda.amp.GradScaler() if (args.amp and torch.cuda.is_available()) else None
        amp_enabled = (scaler is not None)

        best_acc = 0.0
        best_loss = float("inf")
        best_ckpt = None

        for epoch in range(1, args.epochs + 1):
            train_one_epoch(model, device, train_loader, criterion, optimizer, epoch, args.epochs, scaler=scaler, amp=amp_enabled)
            val_loss, val_acc = validate(model, device, test_loader, criterion, epoch, args.epochs)

            # scheduler step
            try:
                scheduler.step()
            except Exception:
                pass

            # checkpoint best by acc
            if val_acc > best_acc:
                best_acc = val_acc
                try:
                    ckpt_path = os.path.join(script_dir, "best_model.pth")
                    torch.save({"epoch": epoch, "model_state": model.state_dict(), "optimizer_state": optimizer.state_dict(), "val_acc": val_acc}, ckpt_path)
                    best_ckpt = ckpt_path
                    console.print(f"[green]Saved improved model to {ckpt_path} (acc {best_acc:.2f}%)[/green]")
                except Exception as e:
                    console.print(f"[red]Failed to save checkpoint: {e}[/red]")

            best_loss = min(best_loss, val_loss)

        # final prints
        console.print(f"[bold]Best validation accuracy:[/] {best_acc:.2f}%")
        console.print(f"[bold]Last validation accuracy:[/] {val_acc:.2f}%")
        if best_ckpt:
            console.print(f"[bold]Best checkpoint:[/] {best_ckpt}")

        print(f"VAL_LOSS: {val_loss}")
        print(f"VAL_ACC: {val_acc}")

        end_time = time.time()
        runtime_seconds = end_time - start_time

        print(f"RUNTIME: {runtime_seconds:.3f}")
        max_runtime = 7200
        normalized_runtime = (runtime_seconds / max_runtime) * 100
        normalized_runtime = round(normalized_runtime, 3)
        print(f"NORMALIZED_RUNTIME: {normalized_runtime}")

    except KeyboardInterrupt:
        graceful_exit(None, None)
    except Exception as e:
        console.print(f"[bold red]An error occurred: {e}[/bold red]")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
