#!/usr/bin/env python3

import sys
import os
import platform
import shutil
import subprocess
import signal
import traceback
from pathlib import Path
import hashlib
import time

start_time = time.time()

os.environ["CUDA_LAUNCH_BLOCKING"] = "1"

SCRIPT_DIR = Path(__file__).resolve().parent

try:
    import venv
except ModuleNotFoundError:
    print("venv not found. Is python3-venv installed?")
    sys.exit(1)

clusterhost = os.environ.get("CLUSTERHOST", "").encode()
hash_suffix = hashlib.sha256(clusterhost).hexdigest()[:10]
VENV_PATH = SCRIPT_DIR / f".torch_venv_{hash_suffix}"

if platform.system() == "Windows":
    PYTHON_BIN = VENV_PATH / "Scripts" / "python.exe"
else:
    PYTHON_BIN = VENV_PATH / "bin" / "python"

def create_and_setup_venv():
    venv.create(VENV_PATH, with_pip=True)
    subprocess.check_call([str(PYTHON_BIN), "-m", "pip", "install", "--upgrade", "pip"])
    subprocess.check_call([str(PYTHON_BIN), "-m", "pip", "install", "rich", "torch", "torchvision"])

def restart_with_venv():
    try:
        result = subprocess.run([str(PYTHON_BIN)] + sys.argv, text=True, check=True, env=dict(**os.environ))
        sys.exit(result.returncode)
    except subprocess.CalledProcessError as e:
        sys.exit(e.returncode)
    except KeyboardInterrupt:
        sys.exit(0)
    except Exception as e:
        print(f"Unexpected error while restarting python: {e}")
        sys.exit(1)

def ensure_venv_and_rich():
    try:
        import rich  # noqa: F401
        import torch  # noqa: F401
        import torchvision  # noqa: F401
    except (OSError, ModuleNotFoundError):
        if not VENV_PATH.exists():
            create_and_setup_venv()
        else:
            try:
                subprocess.check_call([str(PYTHON_BIN), "-m", "pip", "install", "-q", "rich", "torch", "torchvision"])
            except subprocess.CalledProcessError:
                shutil.rmtree(VENV_PATH)
                create_and_setup_venv()
        restart_with_venv()
    except KeyboardInterrupt:
        sys.exit(0)

ensure_venv_and_rich()

import argparse
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from torchvision import datasets, transforms
from rich.console import Console
from rich.progress import Progress, TextColumn, BarColumn, TaskProgressColumn, TimeRemainingColumn
from rich.panel import Panel
from rich.table import Table
from rich import box

console = Console()

shown_exit_msg = False

def graceful_exit(signum, frame):
    global shown_exit_msg

    if not shown_exit_msg:
        console.print("\n[bold red]You pressed Ctrl+C. Exiting gracefully.[/bold red]")
        shown_exit_msg = True

    sys.exit(0)

signal.signal(signal.SIGINT, graceful_exit)

def parse_args():
    parser = argparse.ArgumentParser(description="Train a simple neural network on CIFAR100 with CLI hyperparameters.")

    # Training hyperparameters
    parser.add_argument("--epochs", type=int, default=60, help="Number of epochs to train. (Default bumped to 60 for CIFAR100)")
    parser.add_argument("--batch_size", type=int, default=128, help="Batch size for training and testing.")
    parser.add_argument("--learning_rate", type=float, default=1e-3, help="Learning rate for optimizer.")
    parser.add_argument("--optimizer", type=str, choices=["sgd", "adam", "rmsprop"], default="adam", help="Optimizer to use.")
    parser.add_argument("--momentum", type=float, default=0.9, help="Momentum for SGD optimizer.")
    parser.add_argument("--weight_decay", type=float, default=1e-4, help="L2 weight decay (regularization).")

    # MLP / Dense layer hyperparameters
    parser.add_argument("--hidden_size", type=int, default=128, help="Number of neurons in the hidden layer.")
    parser.add_argument("--dropout", type=float, default=0.2, help="Dropout probability (0.0 = no dropout).")
    parser.add_argument("--activation", type=str, choices=["relu", "tanh", "leaky_relu", "sigmoid"], default="relu", help="Activation function to use.")
    parser.add_argument("--init", type=str, choices=["xavier", "kaiming", "normal", "None"], default="kaiming", help="Weight initialization method.")
    parser.add_argument("--num_dense_layers", type=int, default=2, help="Number of dense (fully-connected) layers excluding the output layer.")

    # Convolutional layer hyperparameters (neu)
    parser.add_argument("--filter", type=int, default=16, help="Number of filters per Conv2D layer")
    parser.add_argument("--num_conv_layers", type=int, default=4, help="Number of convolutional layers")

    # Conv internals (defaults better tuned)
    parser.add_argument("--conv_kernel_size", type=int, default=3, help="Conv kernel size")
    parser.add_argument("--conv_stride", type=int, default=1, help="Conv stride (use 1 and pool instead of large stride)")
    parser.add_argument("--conv_padding", type=int, default=1, help="Conv padding (for kernel 3 -> 1)")

    # Misc
    parser.add_argument("--seed", type=int, default=None, help="Random seed.")
    parser.add_argument("--install", action="store_true", help="Install required dependencies (default: False)")

    return parser.parse_args()

class SimpleMLP(nn.Module):
    def __init__(self,
                 input_channels,
                 input_height,
                 input_width,
                 num_classes,
                 hidden_size,
                 dropout,
                 activation,
                 init_mode,
                 num_dense_layers,
                 num_conv_layers,
                 conv_filters,
                 conv_kernel_size=3,
                 conv_stride=1,
                 conv_padding=1):
        super(SimpleMLP, self).__init__()

        self.activation_fn = self.get_activation(activation)
        self.dropout = nn.Dropout(p=dropout)

        # Convolutional Layers â€” build blocks of conv->bn->act and after each 2 convs add MaxPool2d(2)
        conv_layers = []
        in_channels = input_channels
        h, w = input_height, input_width

        for i in range(num_conv_layers):
            conv_layers.append((f"conv{i}", nn.Conv2d(in_channels, conv_filters,
                                                      kernel_size=conv_kernel_size,
                                                      stride=conv_stride,
                                                      padding=conv_padding)))
            conv_layers.append((f"bn{i}", nn.BatchNorm2d(conv_filters)))
            conv_layers.append((f"act_conv{i}", self.get_activation(activation)))
            in_channels = conv_filters
            # compute output spatial size after conv
            h = (h + 2*conv_padding - conv_kernel_size)//conv_stride + 1
            w = (w + 2*conv_padding - conv_kernel_size)//conv_stride + 1

            # after every 2 conv layers, add MaxPool2d to reduce spatial dims by 2
            if (i + 1) % 2 == 0:
                conv_layers.append((f"pool{(i+1)//2}", nn.MaxPool2d(kernel_size=2, stride=2)))
                h = h // 2
                w = w // 2

        # convert list of tuples into Sequential with ordered names
        conv_seq = nn.Sequential()
        for name, module in conv_layers:
            conv_seq.add_module(name, module)

        self.conv = conv_seq
        flattened_size = in_channels * max(1, h) * max(1, w)

        # Dense Layers
        dense_layers = nn.Sequential()
        in_features = flattened_size
        for i in range(num_dense_layers):
            dense_layers.add_module(f"fc{i}", nn.Linear(in_features, hidden_size))
            dense_layers.add_module(f"act{i}", self.get_activation(activation))
            dense_layers.add_module(f"dropout{i}", nn.Dropout(p=dropout))
            in_features = hidden_size

        dense_layers.add_module("output", nn.Linear(in_features, num_classes))

        self.classifier = dense_layers

        # Initialization
        if init_mode is not None and init_mode != "None":
            self.init_weights(init_mode)

    def forward(self, x):
        x = self.conv(x)
        x = x.view(x.size(0), -1)
        return self.classifier(x)

    def get_activation(self, name):
        name = name.lower()
        if name == "relu":
            return nn.ReLU()
        elif name == "tanh":
            return nn.Tanh()
        elif name == "sigmoid":
            return nn.Sigmoid()
        elif name == "leaky_relu":
            return nn.LeakyReLU(negative_slope=0.01)
        else:
            raise ValueError(f"Unbekannte Aktivierung: {name}")

    def init_weights(self, mode):
        for m in self.modules():
            if isinstance(m, nn.Linear) or isinstance(m, nn.Conv2d):
                if mode == "xavier":
                    nn.init.xavier_uniform_(m.weight)
                elif mode == "kaiming":
                    # Kaiming for ReLU-style activations
                    nn.init.kaiming_uniform_(m.weight, nonlinearity='relu')
                elif mode == "normal":
                    nn.init.normal_(m.weight, mean=0.0, std=0.02)
                else:
                    raise ValueError(f"Unbekannte Init-Methode: {mode}")
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)

def show_hyperparams(args):
    table = Table(title="Hyperparameters", box=box.ROUNDED, title_style="bold magenta")
    table.add_column("Parameter", style="bold cyan")
    table.add_column("Value", style="bold green")

    table.add_row("Epochs", str(args.epochs))
    table.add_row("Num Dense Layers", str(args.num_dense_layers))
    table.add_row("Batch size", str(args.batch_size))
    table.add_row("Learning rate", str(args.learning_rate))
    table.add_row("Hidden size", str(args.hidden_size))
    table.add_row("Dropout", str(args.dropout))
    table.add_row("Optimizer", args.optimizer)
    table.add_row("Momentum", str(args.momentum))
    table.add_row("Weight Decay", str(args.weight_decay))
    table.add_row("Activation", args.activation)
    table.add_row("Init Method", args.init)
    table.add_row("Seed", str(args.seed) if args.seed is not None else "None")
    table.add_row("Conv Filters", str(args.filter))
    table.add_row("Num Conv Layers", str(args.num_conv_layers))
    table.add_row("Conv Kernel", str(args.conv_kernel_size))
    table.add_row("Conv Stride", str(args.conv_stride))
    table.add_row("Conv Padding", str(args.conv_padding))

    console.print(table)

def print_model_summary(model, input_size):
    device = next(model.parameters()).device
    x = torch.zeros(1, *input_size).to(device)

    table = Table(
        title="Model Summary",
        box=box.ROUNDED,
        title_style="bold magenta"
    )
    table.add_column("Layer", style="bold cyan")
    table.add_column("Output Shape", style="bold green")
    table.add_column("Param #", justify="right", style="bold yellow")

    total_params = 0

    # iterate through conv submodules in order
    for name, layer in model.conv._modules.items():
        x = layer(x)
        params = sum(p.numel() for p in layer.parameters() if p.requires_grad)
        total_params += params
        table.add_row(f"conv::{name}", str(list(x.shape)), str(params))

    # Flatten for dense layers
    x = x.view(x.size(0), -1)

    # iterate dense
    for name, layer in model.classifier._modules.items():
        x = layer(x)
        params = sum(p.numel() for p in layer.parameters() if p.requires_grad)
        total_params += params
        out_shape_str = str(list(x.shape))
        table.add_row(f"dense::{name}", out_shape_str, str(params))

    # Gesamtparameter
    table.add_row("Total", "-", str(total_params))

    console.print(table)

def train(model, device, train_loader, criterion, optimizer, epoch, total_epochs):
    model.train()
    running_loss = 0.0
    correct = 0
    total = 0

    console.rule(f"[bold blue]Epoch {epoch}/{total_epochs} - Training")

    with Progress(
        TextColumn("[bold blue]Batch {task.completed}/{task.total}"),
        BarColumn(),
        TaskProgressColumn(),
        TimeRemainingColumn(),
        TextColumn("Loss: [green]{task.fields[loss]:.4f}"),
        TextColumn("Acc: [cyan]{task.fields[acc]:.2f}%"),
        transient=True,
    ) as progress:
        task_id = progress.add_task("Training", total=len(train_loader), loss=0.0, acc=0.0)

        for batch_idx, (data, target) in enumerate(train_loader):
            data, target = data.to(device), target.to(device)

            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()

            running_loss += loss.item()
            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

            progress.update(
                task_id,
                advance=1,
                loss=running_loss / (batch_idx + 1),
                acc=100.0 * correct / total
            )

    console.print(f"[green]Epoch-Loss[/green]: {running_loss:.4f}")

def test(model, device, test_loader, criterion, epoch, total_epochs):
    model.eval()
    test_loss = 0.0
    correct = 0
    total = 0

    console.rule(f"[bold magenta]Epoch {epoch}/{total_epochs} - Validation")

    with torch.no_grad():
        for data, target in test_loader:
            data, target = data.to(device), target.to(device)
            output = model(data)
            loss = criterion(output, target)
            test_loss += loss.item()

            _, predicted = torch.max(output.data, 1)
            total += target.size(0)
            correct += (predicted == target).sum().item()

    if len(test_loader) > 0:
        test_loss /= len(test_loader)
    accuracy = 100.0 * correct / total if total > 0 else 0.0

    console.print(
        Panel.fit(
            f"[bold green]Validation Loss:[/] {test_loss:.4f}\n"
            f"[bold green]Accuracy:[/] {accuracy:.2f}%",
            title=f"Epoch {epoch}/{total_epochs} Summary",
            box=box.DOUBLE
        )
    )

    return test_loss, accuracy

def main():
    try:
        args = parse_args()

        show_hyperparams(args)

        if args.seed is not None:
            torch.manual_seed(args.seed)
            torch.cuda.manual_seed_all(args.seed)

        # device + speed tweaks
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        if torch.backends.cudnn.is_available():
            torch.backends.cudnn.benchmark = True

        # Transforms: TRAIN (with augmentation) vs TEST (deterministic)
        train_transform = transforms.Compose([
            transforms.RandomCrop(32, padding=4),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize(
                (0.5071, 0.4867, 0.4408),
                (0.2675, 0.2565, 0.2761)
            )
        ])

        test_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(
                (0.5071, 0.4867, 0.4408),
                (0.2675, 0.2565, 0.2761)
            )
        ])

        script_dir = os.path.dirname(__file__)
        data_dir = os.path.join(script_dir, "cifar")

        # download datasets
        train_dataset = datasets.CIFAR100(root=data_dir, train=True, download=True, transform=train_transform)
        test_dataset = datasets.CIFAR100(root=data_dir, train=False, download=True, transform=test_transform)

        if args.install:
            print("Exiting, since the installation should now be done")
            sys.exit(0)

        # DataLoader performance options
        num_workers = min(4, (os.cpu_count() or 1))
        pin_memory = True if torch.cuda.is_available() else False

        train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True, num_workers=num_workers, pin_memory=pin_memory)
        test_loader = DataLoader(test_dataset, batch_size=args.batch_size, shuffle=False, num_workers=num_workers, pin_memory=pin_memory)

        model = SimpleMLP(
            input_height=32,
            input_width=32,
            input_channels=3,
            hidden_size=args.hidden_size,
            num_classes=100,
            dropout=args.dropout,
            activation=args.activation,
            init_mode=args.init,
            num_dense_layers=args.num_dense_layers,
            num_conv_layers=args.num_conv_layers,
            conv_filters=args.filter,
            conv_kernel_size=args.conv_kernel_size,
            conv_stride=args.conv_stride,
            conv_padding=args.conv_padding
        ).to(device)

        print_model_summary(model, input_size=(3, 32, 32))

        if args.learning_rate == 0:
            console.print("[red]Learning Rate cannot be 0[/red]")
            sys.exit(1)

        if args.optimizer == "adam":
            optimizer = torch.optim.Adam(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
        elif args.optimizer == "sgd":
            optimizer = torch.optim.SGD(model.parameters(), lr=args.learning_rate, momentum=args.momentum, weight_decay=args.weight_decay)
        elif args.optimizer == "rmsprop":
            optimizer = torch.optim.RMSprop(model.parameters(), lr=args.learning_rate, weight_decay=args.weight_decay)
        else:
            raise ValueError(f"Unknown optimizer: {args.optimizer}")

        # Scheduler: StepLR is simple and effective; you can swap to ReduceLROnPlateau if desired.
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)

        criterion = nn.CrossEntropyLoss()

        best_acc = 0.0
        best_loss = float("inf")
        for epoch in range(1, args.epochs + 1):
            train(model, device, train_loader, criterion, optimizer, epoch, args.epochs)
            val_loss, val_acc = test(model, device, test_loader, criterion, epoch, args.epochs)

            # step scheduler (for StepLR)
            scheduler.step()

            # save checkpoint if improved (optional)
            #if val_acc > best_acc:
            #    best_acc = val_acc
            #    try:
            #        ckpt_path = os.path.join(script_dir, "best_model.pth")
            #        torch.save({"epoch": epoch, "model_state": model.state_dict(), "optimizer_state": optimizer.state_dict()}, ckpt_path)
            #        console.print(f"[green]Saved improved model to {ckpt_path} (acc {best_acc:.2f}%)[/green]")
            #    except Exception as e:
            #        console.print(f"[red]Failed to save checkpoint: {e}[/red]")

            best_loss = min(best_loss, val_loss)

        print(f"VAL_LOSS: {val_loss}")
        print(f"VAL_ACC: {val_acc}")

        end_time = time.time()
        runtime_seconds = end_time - start_time

        print(f"RUNTIME: {runtime_seconds:.3f}")
        max_runtime = 7200
        normalized_runtime = (runtime_seconds / max_runtime) * 100
        normalized_runtime = round(normalized_runtime, 3)
        print(f"NORMALIZED_RUNTIME: {normalized_runtime}")

    except KeyboardInterrupt:
        graceful_exit(None, None)
    except Exception as e:
        console.print(f"[bold red]An error occurred: {e}[/bold red]")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
